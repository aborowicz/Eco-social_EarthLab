{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started\n",
    "This notebook uses iPython Parallel to crawl multiple sites simultaneously. To get started, we'll have to first setup our environment. We need to connect to the iPython cluster we setup on SeaWulf and verify that all of the processors we requested are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors available: 0\n"
     ]
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "c = ipp.Client()\n",
    "print('Number of processors available: ' + str(len(c.ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that MPI is working using the parallel cell magic command \"%px\" and some simple code.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Cell magic `%%px` not found.\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "from mpi4py import MPI\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "print(\"Processor \" + str(rank) + \" reporting for duty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limits\n",
    "\n",
    "The Web crawlers we use are modified versions of the Python libraries iCrawler (https://pypi.python.org/pypi/icrawler/0.4.7) and InstagramCrawler (https://github.com/iammrhelo/InstagramCrawler).\n",
    "\n",
    "### Baidu, Bing, Flickr, Google, and Photobucket\n",
    "\n",
    "The iCrawler-based script is used to scrape the sites listed above. The limits for each of these sites are:\n",
    "\n",
    "* Baidu, Bing, and Google limit you to the first 1000 search results per crawl.\n",
    "* Photobucket's top-100 RSS feed only contains 100 images (surprise!)\n",
    "* Flickr limits you to 3600 images per hour, requires that you obtain an API key, and will ban the key if you exceed this limit.\n",
    "\n",
    "### Instagram and Smugmug\n",
    "\n",
    "The InstagramCrawler-based scripts can be used to scrape both Instagram and Smugmug. Instead of using each of these sites APIs, these scripts mimic a human user that is very polite and doesn't exceed acesss limits. Since they don't use any official API, there aren't strict limits on the number of images you can download. However, since it mimics a human user with explicit sleep timers, it can take significantly longer than the iCrawler-based script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling the Web\n",
    "\n",
    "The following block of code is setup to crawl each site you select in parallel. You can modify the variable keyword to change the keyword you search each site for, maxnum to change the maximum number of images you will download from any Website, and crawlers to select which sites you will crawl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Cell magic `%%px` not found.\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "# keyword to search for\n",
    "keyword = 'seal'\n",
    "\n",
    "# maximum number of images to download per Website\n",
    "maxnum = 100\n",
    "\n",
    "# list of Websites to crawl. format should be an array of lowercase strings\n",
    "# available options are: baidu, bing, flickr, google, instagram, photobucket, smugmug\n",
    "crawlers = ['baidu', 'bing', 'google', 'photobucket', 'smugmug']\n",
    "\n",
    "# your Flickr API key\n",
    "flickr_api = False\n",
    "\n",
    "from mpi4py import MPI\n",
    "# we sometimes get warnings for invalid images. these are annoying at best.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = comm.Get_rank()\n",
    "\n",
    "if (rank < len(crawlers)):\n",
    "    site = crawlers[rank]\n",
    "    print('Processor ' + str(rank) + ' crawling ' + site.title())\n",
    "    if site in ['baidu','bing','google']:\n",
    "        maxnum = min(maxnum, 1000) # these sites limit you to 1000 images per search\n",
    "        %run ./crawlers/icrawl.py $keyword $maxnum -c $site\n",
    "    elif site == 'flickr':\n",
    "        if not flickr_api:\n",
    "            print('You must provide your Flickr API key in the flickr_api variable above to crawl Flickr.')\n",
    "        else:\n",
    "            %run ./crawlers/icrawl.py $keyword $maxnum -c $site\n",
    "    elif site == 'photobucket':\n",
    "        maxnum = min(maxnum, 100) # only 100 images max on this site\n",
    "        %run ./crawlers/icrawl.py $keyword $maxnum -c $site\n",
    "    elif site == 'instagram':\n",
    "        %run ./crawlers/instagramcrawler.py $keyword $maxnum\n",
    "    elif site == 'smugmug':\n",
    "        %run ./crawlers/smugmugcrawler.py $keyword $maxnum\n",
    "    else:\n",
    "        print('\"' + site + '\" is an invalid crawler.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing Images\n",
    "\n",
    "If you ran the above code block with properly set variables, you should have downloaded images into the /seals_geo_survey/images directory. You can confirm the creation of these directories using the %ls magic command. Feel free to modify the directory name to navigate further (e.g. %ls ~/seals_geo_survey/images/google)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/home/bento/seals_geo_survey/images/google': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "%ls ~/seals_geo_survey/images/google"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the code block below to view a specific image by filling in the image_path variable. (e.g. image_path = './images/google/fuzzy_seal.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_path = './images/\n",
    "\n",
    "from IPython.display import Image\n",
    "Image(filename=image_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "When you crawl Instagram for \"seal\" or \"seals\" you get around 750k posts but a lot of Navy seals and other invalid images. If you search for \"furseal\" or \"furseals\" you get better results. Similar things happen with the other crawlers. I would recommend performing a manual search to assess the content and adjust your keywords before crawling.\n",
    "\n",
    "You can specify multiple keywords for the search engine sites, such as keyword = 'antarctica+seal'.\n",
    "\n",
    "Images are automatically downloaded into the ~/seals_geo_survey/images directory. Future stages of the pipeline depend on this structure, so do not move anything around manually."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
